{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final Project - Computing for Data Science**\n",
    "\n",
    "## Group Members:\n",
    "1. **Tarang Kadyan**  \n",
    "   <tarang.kadyan@bse.eu>\n",
    "\n",
    "2. **Deepak Malik**  \n",
    "   <deepak.malik@bse.eu>\n",
    "\n",
    "3. **Enzo Infantes**  \n",
    "   <enzo.infantes@bse.eu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('c:/Users/Enzo/Documents/BSE/T1/COMPUTING_DS/Final_Project/Final-Project/telco-churn-library')\n",
    "\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "from telco_churn.data_preprocessing.data_loader import CSVDataLoader, DataPreparer\n",
    "from telco_churn.data_preprocessing.preprocessor import HandleMissingValues,NormalizeData, EncodeCategoricalData, HandleOutliers,PreprocessingPipeline\n",
    "from telco_churn.feature_engineering.feature_engineering import StatisticalFeatures, CategoricalEncoding, InteractionFeatures, TemporalFeatures, DerivedFeatures, FeaturePipeline\n",
    "from telco_churn.modelling.model import LogisticRegressionModel, HyperparameterTuner, ModelingPipeline, CrossValidator\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1: Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from C:\\Users\\Enzo\\.cache\\kagglehub\\datasets\\blastchar\\telco-customer-churn\\versions\\1\\WA_Fn-UseC_-Telco-Customer-Churn.csv.\n",
      "Column 'TotalCharges' successfully converted to numeric and blanks replaced with NaN.\n",
      "Data validation successful.\n"
     ]
    }
   ],
   "source": [
    "# Define our file path to upload the data\n",
    "path = kagglehub.dataset_download(\"blastchar/telco-customer-churn\")\n",
    "file_path = os.path.join(path, 'WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "required_columns = ['gender', 'tenure', 'MonthlyCharges', 'TotalCharges', 'Churn']\n",
    "csv_loader = CSVDataLoader(required_columns=required_columns)\n",
    "\n",
    "preparer = DataPreparer(loaders=[csv_loader])\n",
    "\n",
    "data = preparer.load_and_validate(file_path, loader_type=\"csv\", column_to_convert=\"TotalCharges\")\n",
    "\n",
    "# Only to make the process faster with the most important information\n",
    "cols = ['gender', 'SeniorCitizen', 'Partner', 'tenure', 'PhoneService', \n",
    "        'PaperlessBilling', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "X = data[cols]\n",
    "y = data['Churn'].map({'Yes': 1, 'No': 0}).squeeze() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2: Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PreprocessingPipeline(preprocessors=[\n",
    "           HandleMissingValues(strategy='mean'),  # Impute missing values with the mean\n",
    "           NormalizeData(method='minmax'),        # Normalize data using MinMax scaling\n",
    "           EncodeCategoricalData(),               # One-hot encode categorical variables\n",
    "           HandleOutliers(method='iqr')           # Handle outliers using IQR method\n",
    "])\n",
    "\n",
    "processed_data = pipeline.apply(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3: Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = FeaturePipeline(transformers=[\n",
    "           StatisticalFeatures(group_by_column=None),\n",
    "           CategoricalEncoding(),\n",
    "           InteractionFeatures(),\n",
    "           TemporalFeatures(),\n",
    "           DerivedFeatures()\n",
    "])\n",
    "\n",
    "X = pipeline.apply(processed_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 4: Split, Train, and Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hyperparameter tuning...\n",
      "Best Parameters: {'C': 10, 'max_iter': 100}\n",
      "Model Performance:\n",
      "Accuracy: 0.7913\n",
      "Precision: 0.6436\n",
      "Recall: 0.4745\n",
      "F1_score: 0.5463\n",
      "Roc_auc: 0.8390\n",
      "\n",
      "Model Performance Metrics:\n",
      "Accuracy: 0.7913\n",
      "Precision: 0.6436\n",
      "Recall: 0.4745\n",
      "F1_score: 0.5463\n",
      "Roc_auc: 0.8390\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LogisticRegression model\n",
    "log_reg_model = LogisticRegressionModel(solver='liblinear')\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'max_iter': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Initialize the hyperparameter tuner\n",
    "hyperparameter_tuner = HyperparameterTuner(model=log_reg_model.model, param_grid=param_grid)\n",
    "\n",
    "# Create the modeling pipeline with the model and tuner\n",
    "modeling_pipeline = ModelingPipeline(model=log_reg_model, tuner=hyperparameter_tuner)\n",
    "\n",
    "# Run the pipeline\n",
    "metrics = modeling_pipeline.run(X, y)\n",
    "\n",
    "# Print metrics (this will include accuracy, precision, recall, f1_score, roc_auc)\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 5: Save The Best Model - API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trained_model.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = log_reg_model.model\n",
    "joblib.dump(best_model, \"trained_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
