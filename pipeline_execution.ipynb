{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from /Users/tarangkadyan/Downloads/telco_churn_library/Data/data.csv.\n",
      "Data validation successful.\n",
      "   SeniorCitizen    tenure  MonthlyCharges  customerID_0002-ORFBO  \\\n",
      "0            0.0  0.013889        0.115423                    0.0   \n",
      "1            0.0  0.472222        0.385075                    0.0   \n",
      "2            0.0  0.027778        0.354229                    0.0   \n",
      "3            0.0  0.625000        0.239303                    0.0   \n",
      "4            0.0  0.027778        0.521891                    0.0   \n",
      "\n",
      "   customerID_0003-MKNFE  customerID_0004-TLHLJ  customerID_0011-IGKFF  \\\n",
      "0                    0.0                    0.0                    0.0   \n",
      "1                    0.0                    0.0                    0.0   \n",
      "2                    0.0                    0.0                    0.0   \n",
      "3                    0.0                    0.0                    0.0   \n",
      "4                    0.0                    0.0                    0.0   \n",
      "\n",
      "   customerID_0013-EXCHZ  customerID_0013-MHZWF  customerID_0013-SMEOE  ...  \\\n",
      "0                    0.0                    0.0                    0.0  ...   \n",
      "1                    0.0                    0.0                    0.0  ...   \n",
      "2                    0.0                    0.0                    0.0  ...   \n",
      "3                    0.0                    0.0                    0.0  ...   \n",
      "4                    0.0                    0.0                    0.0  ...   \n",
      "\n",
      "   TotalCharges_996.85  TotalCharges_996.95  TotalCharges_997.65  \\\n",
      "0                  0.0                  0.0                  0.0   \n",
      "1                  0.0                  0.0                  0.0   \n",
      "2                  0.0                  0.0                  0.0   \n",
      "3                  0.0                  0.0                  0.0   \n",
      "4                  0.0                  0.0                  0.0   \n",
      "\n",
      "   TotalCharges_997.75  TotalCharges_998.1  TotalCharges_999.45  \\\n",
      "0                  0.0                 0.0                  0.0   \n",
      "1                  0.0                 0.0                  0.0   \n",
      "2                  0.0                 0.0                  0.0   \n",
      "3                  0.0                 0.0                  0.0   \n",
      "4                  0.0                 0.0                  0.0   \n",
      "\n",
      "   TotalCharges_999.8  TotalCharges_999.9  Churn_No  Churn_Yes  \n",
      "0                 0.0                 0.0       1.0        0.0  \n",
      "1                 0.0                 0.0       1.0        0.0  \n",
      "2                 0.0                 0.0       0.0        1.0  \n",
      "3                 0.0                 0.0       1.0        0.0  \n",
      "4                 0.0                 0.0       0.0        1.0  \n",
      "\n",
      "[5 rows x 13620 columns]\n"
     ]
    }
   ],
   "source": [
    "# Add the src_telco_churn folder to the system path\n",
    "import sys\n",
    "sys.path.append('./src_telco_churn')\n",
    "\n",
    "# Now import the necessary modules\n",
    "from src_telco_churn.data_loader import CSVDataLoader, DataPreparer  \n",
    "from src_telco_churn.modeling import CrossValidator, TrainTestSplit, LogisticRegressionModel, HyperparameterTuner, ModelingPipeline\n",
    "from src_telco_churn.preprocessor import HandleMissingValues, NormalizeData, EncodeCategoricalData, HandleOutliers,PreprocessingPipeline\n",
    "from src_telco_churn.feature_engineering import (StatisticalFeatures, CategoricalEncoding, InteractionFeatures, TemporalFeatures, DerivedFeatures,FeaturePipeline)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the data\n",
    "# Initialize the CSVDataLoader with required columns\n",
    "required_columns = ['gender', 'tenure', 'MonthlyCharges', 'TotalCharges', 'Churn']  # Adjust to your dataset\n",
    "csv_loader = CSVDataLoader(required_columns=required_columns)\n",
    "preparer = DataPreparer(loaders=[csv_loader])\n",
    "                        \n",
    "# Load and validate the dataset\n",
    "file_path = '/Users/tarangkadyan/Downloads/telco_churn_library/Data/data.csv'  # Provide the correct path to your data\n",
    "data = preparer.load_and_validate(file_path, loader_type=\"csv\")\n",
    "data.head()\n",
    "\n",
    "#Step-2:  Preprocessing data\n",
    "# Initialize the preprocessing pipeline\n",
    "pipeline = PreprocessingPipeline(preprocessors=[\n",
    "    HandleMissingValues(strategy='mean'),  # Impute missing values with the mean\n",
    "    NormalizeData(method='minmax'),         # Normalize data using MinMax scaling\n",
    "    EncodeCategoricalData(),               # One-hot encode categorical variables\n",
    "    HandleOutliers(method='iqr')           # Handle outliers using IQR method\n",
    "])\n",
    "\n",
    "# Apply preprocessing to the data\n",
    "processed_data = pipeline.apply(data)\n",
    "\n",
    "# Display the preprocessed data\n",
    "print(processed_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature Engineering\n",
    "pipeline = FeaturePipeline(transformers=[\n",
    "    StatisticalFeatures(group_by_column=None),\n",
    "    CategoricalEncoding(),\n",
    "    InteractionFeatures(),\n",
    "    TemporalFeatures(),\n",
    "    DerivedFeatures()\n",
    "])\n",
    "\n",
    "# Apply feature engineering to the processed data\n",
    "df_transformed = pipeline.apply(processed_data)\n",
    "\n",
    "# Step 4: Split the data into train and test sets\n",
    "features_data = df_transformed  # Use the feature-engineered data\n",
    "X = features_data.drop(columns='Churn')  # Adjust target column name as needed\n",
    "y = features_data['Churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train the model\n",
    "model_trainer = ModelTrainer()\n",
    "model = model_trainer.train_model(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "model_evaluator = ModelEvaluator(model)\n",
    "evaluation_results = model_evaluator.evaluate_model(X_test, y_test)\n",
    "print(evaluation_results)\n",
    "\n",
    "# Step 7: Predict on the test set\n",
    "predictions = model.predict(X_test)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FeatureEngineering' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 3: Feature Engineering\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Initialize the Feature Engineering Pipeline (adjust as per your requirements)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m feature_engineering \u001b[38;5;241m=\u001b[39m FeaturePipeline([\n\u001b[0;32m----> 4\u001b[0m     FeatureEngineering()  \u001b[38;5;66;03m# Add all feature transformers here\u001b[39;00m\n\u001b[1;32m      5\u001b[0m ])\n\u001b[1;32m      6\u001b[0m features_data \u001b[38;5;241m=\u001b[39m feature_engineering\u001b[38;5;241m.\u001b[39mapply(processed_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FeatureEngineering' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 3: Feature Engineering\n",
    "# Initialize the Feature Engineering Pipeline\n",
    "# Example usage of the FeaturePipeline\n",
    "pipeline = FeaturePipeline(transformers=[\n",
    "    StatisticalFeatures(group_by_column='group'),\n",
    "    CategoricalEncoding(),\n",
    "    InteractionFeatures(),\n",
    "    TemporalFeatures(),\n",
    "    DerivedFeatures()\n",
    "])\n",
    "\n",
    "df_transformed = pipeline.apply(df)  # Apply the transformations to your dataframe\n",
    "\n",
    "\n",
    "# Step 4: Split the data into train and test sets\n",
    "X = features_data.drop(columns='target_column')  # Adjust 'target_column' as per your dataset\n",
    "y = features_data['target_column']  # Adjust 'target_column' as per your dataset\n",
    "\n",
    "# Split the data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train the model\n",
    "# Initialize the ModelTrainer\n",
    "model_trainer = ModelTrainer()\n",
    "model = model_trainer.train_model(X_train, y_train)  # Train the model with training data\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "# Initialize the ModelEvaluator\n",
    "model_evaluator = ModelEvaluator(model)\n",
    "evaluation_results = model_evaluator.evaluate_model(X_test, y_test)  # Evaluate on test data\n",
    "print(evaluation_results)\n",
    "\n",
    "# Step 7: Predict on the test set\n",
    "predictions = model.predict(X_test)  # Get predictions (probabilities) on the test set\n",
    "print(predictions)\n",
    "\n",
    "# Step 8: Hyperparameter Tuning (optional)\n",
    "# Initialize ModelTuning and perform hyperparameter tuning (if desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Split the data into train and test sets\n",
    "X = features_data.drop(columns='target_column')  # Adjust 'target_column' as per your dataset\n",
    "y = features_data['target_column']  # Adjust 'target_column' as per your dataset\n",
    "\n",
    "# Split the data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train the model\n",
    "# Initialize the ModelTrainer\n",
    "model_trainer = ModelTrainer()\n",
    "model = model_trainer.train_model(X_train, y_train)  # Train the model with training data\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "# Initialize the ModelEvaluator\n",
    "model_evaluator = ModelEvaluator(model)\n",
    "evaluation_results = model_evaluator.evaluate_model(X_test, y_test)  # Evaluate on test data\n",
    "print(evaluation_results)\n",
    "\n",
    "# Step 7: Predict on the test set\n",
    "predictions = model.predict(X_test)  # Get predictions (probabilities) on the test set\n",
    "print(predictions)\n",
    "\n",
    "# Step 8: Hyperparameter Tuning (optional)\n",
    "# Initialize ModelTuning and perform hyperparameter tuning (if desired)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
